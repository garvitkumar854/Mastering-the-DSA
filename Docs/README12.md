# 12. Time and Space Complexity

Understanding **Time and Space Complexity** is essential for evaluating the efficiency of algorithms. This folder covers the fundamentals of analyzing how the time and memory consumption of code scales with input size `n`.

### 🔹 Time Complexity
Time complexity describes how many **operations** an algorithm performs relative to the size of input. It is not measured in seconds but in terms of how the **number of operations grows**.

Common notations include:
- `O` → Big O (Worst Case)
- `Θ` → Theta (Average Case)
- `Ω` → Omega (Best Case)

### 🔹 Space Complexity
Space complexity refers to the **amount of memory** required by an algorithm, including input and auxiliary space. Efficient algorithms aim to minimize both time and space usage.

You’ll also explore how different algorithms behave, such as:
- O(1): Constant time
- O(log n): Logarithmic
- O(n): Linear
- O(n log n): Linearithmic
- O(n²): Quadratic
- O(2ⁿ), O(n!): Exponential and factorial time (usually in recursive problems)

---

## Table of Contents

- [Concept Files](#concept-files)
  - [Time and Space Complexity Explained](/12_Time&Space_Complexity/01.cpp)
- [Questions](#questions)
  - [Complexity Patterns in Code](/12_Time&Space_Complexity/Qs1.cpp)

---

## 🧠 Learnings

- Understand **why O(n²) is worse than O(n)** in large input cases  
- Estimate runtime even **without executing code**
- Judge the **efficiency of sorting, searching, recursion, and nested loops**
- Make better algorithm choices in interviews and real-world systems

---
