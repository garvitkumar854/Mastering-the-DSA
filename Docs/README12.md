# 12. Time and Space Complexity

Understanding **Time and Space Complexity** is essential for evaluating the efficiency of algorithms. This folder covers the fundamentals of analyzing how the time and memory consumption of code scales with input size `n`.

### ğŸ”¹ Time Complexity
Time complexity describes how many **operations** an algorithm performs relative to the size of input. It is not measured in seconds but in terms of how the **number of operations grows**.

Common notations include:
- `O` â†’ Big O (Worst Case)
- `Î˜` â†’ Theta (Average Case)
- `Î©` â†’ Omega (Best Case)

### ğŸ”¹ Space Complexity
Space complexity refers to the **amount of memory** required by an algorithm, including input and auxiliary space. Efficient algorithms aim to minimize both time and space usage.

Youâ€™ll also explore how different algorithms behave, such as:
- O(1): Constant time
- O(log n): Logarithmic
- O(n): Linear
- O(n log n): Linearithmic
- O(nÂ²): Quadratic
- O(2â¿), O(n!): Exponential and factorial time (usually in recursive problems)

---

## Table of Contents

- [Concept Files](#concept-files)
  - [Time and Space Complexity Explained](/12_Time&Space_Complexity/01.cpp)
- [Questions](#questions)
  - [Complexity Patterns in Code](/12_Time&Space_Complexity/Qs1.cpp)

---

## ğŸ§  Learnings

- Understand **why O(nÂ²) is worse than O(n)** in large input cases  
- Estimate runtime even **without executing code**
- Judge the **efficiency of sorting, searching, recursion, and nested loops**
- Make better algorithm choices in interviews and real-world systems

---
